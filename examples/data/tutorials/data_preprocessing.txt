=================================================================
DATA PREPROCESSING TECHNIQUES FOR MACHINE LEARNING
=================================================================

Understanding Data Preprocessing
-------------------------------
Data preprocessing is a crucial step in any machine learning project. Raw data is often noisy, inconsistent, and may contain many errors. The quality of data and the useful information that can be derived from it directly affects the ability of our model to learn. Therefore, it is extremely important to preprocess the data before feeding it to any machine learning algorithm.

1. DATA CLEANING
----------------
  1.1 Handling Missing Values
      * Deletion: Remove records with missing values
      * Mean/Median/Mode Imputation: Replace missing values with central tendency
      * Predictive Imputation: Use algorithms to predict missing values
      * Advanced methods: KNN imputation, MICE (Multiple Imputation by Chained Equations)
      
  1.2 Handling Outliers
      * Detection: Z-score, IQR method, DBSCAN clustering
      * Treatment: Trimming, Capping, Transformation, Separate modeling
      
  1.3 Handling Inconsistent Data
      * Data validation rules
      * Domain-specific constraints
      * Consistency checks

2. DATA TRANSFORMATION
----------------------
  2.1 Normalization
      * Min-Max Scaling: Scales features to a range, typically [0,1]
      * Example: X_norm = (X - X_min) / (X_max - X_min)
      
  2.2 Standardization
      * Z-score normalization: Transforms data to have zero mean and unit variance
      * Example: X_std = (X - μ) / σ
      
  2.3 Log Transformation
      * Useful for: Skewed data, Reducing impact of outliers
      * Example: X_log = log(X)
      
  2.4 Power Transformation
      * Box-Cox, Yeo-Johnson transformations
      * Stabilizes variance and makes data more normal distribution-like

3. FEATURE ENGINEERING
----------------------
  3.1 Feature Creation
      * Domain knowledge-based features
      * Interaction terms (multiplication, division, addition, subtraction)
      * Polynomial features
      * Date-time features (day of week, month, year, etc.)
      
  3.2 Feature Extraction
      * PCA (Principal Component Analysis)
      * t-SNE (t-Distributed Stochastic Neighbor Embedding)
      * LDA (Linear Discriminant Analysis)
      * Autoencoders
      
  3.3 Text Feature Engineering
      * Bag of Words (BoW)
      * TF-IDF (Term Frequency-Inverse Document Frequency)
      * Word Embeddings (Word2Vec, GloVe, BERT embeddings)

4. CATEGORICAL ENCODING
-----------------------
  4.1 One-Hot Encoding
      * Creates binary columns for each category
      * Suitable for nominal data with no inherent order
      
  4.2 Label Encoding
      * Assigns integers to categories
      * Better for ordinal data or tree-based models
      
  4.3 Target Encoding
      * Replaces categories with the mean of the target variable
      * Good for high-cardinality features
      
  4.4 Binary Encoding
      * Represents categories as binary code
      * Space-efficient for high-cardinality

5. IMBALANCED DATA HANDLING
---------------------------
  5.1 Undersampling
      * Randomly remove samples from the majority class
      * Methods: Random undersampling, Tomek links, NearMiss
      
  5.2 Oversampling
      * Generate synthetic samples for minority class
      * Methods: Random oversampling, SMOTE, ADASYN
      
  5.3 Hybrid Methods
      * Combine undersampling and oversampling
      * Example: SMOTETomek
      
  5.4 Algorithm-level Methods
      * Cost-sensitive learning
      * Ensemble methods with resampling

RELATIONSHIP TO MACHINE LEARNING ALGORITHMS
------------------------------------------
Different preprocessing techniques work better with different types of algorithms:

- Linear Models (Linear Regression, Logistic Regression, SVM)
  * Benefit from standardization/normalization
  * Sensitive to outliers and feature scales
  * Work well with independent features

- Tree-based Models (Decision Trees, Random Forest, XGBoost)
  * Generally robust to feature scaling
  * Can handle missing values and outliers better
  * Benefit from proper categorical encoding
  
- Neural Networks
  * Require standardization/normalization
  * Benefit from regularization techniques
  * Work well with engineered features
  
- Distance-based Models (K-means, KNN)
  * Very sensitive to feature scaling
  * Require normalization/standardization
  * Affected heavily by irrelevant features

PREPROCESSING PIPELINE EXAMPLE
-----------------------------
1. Exploratory Data Analysis
2. Data Cleaning (missing values, outliers)
3. Feature Engineering
4. Feature Selection
5. Data Transformation (scaling, encoding)
6. Train-Test Split
7. Model-specific preprocessing

PREPROCESSING LIBRARIES IN PYTHON
--------------------------------
- Scikit-learn: Preprocessing module includes scalers, encoders
- Pandas: Data manipulation and basic preprocessing
- NumPy: Numerical operations for transformations
- Imbalanced-learn: Tools for imbalanced datasets
- Feature-engine: Advanced feature engineering
- Category_encoders: Advanced categorical encoding methods

COMMON PITFALLS
--------------
- Data leakage during preprocessing
- Preprocessing test data with training statistics
- Inappropriate handling of categorical features
- Not handling imbalanced datasets properly
- Forgetting to scale features for distance-based algorithms
- Over-engineering features leading to overfitting
